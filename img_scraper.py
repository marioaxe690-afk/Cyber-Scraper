import os
import requests
import time
import random
import tkinter as tk
from tkinter import scrolledtext, messagebox
import threading
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse


def find_next_page_link(soup, current_url):
    """æ™ºèƒ½æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥"""
    # å¸¸è§çš„"ä¸‹ä¸€é¡µ"å…³é”®è¯
    next_keywords = ['ä¸‹ä¸€é¡µ', 'ä¸‹é¡µ', 'next', 'Next', 'NEXT', 'â€º', 'Â»', 'â†’']

    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
    all_links = soup.find_all('a', href=True)

    for link in all_links:
        link_text = link.get_text(strip=True)
        link_title = link.get('title', '')
        link_class = ' '.join(link.get('class', []))

        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ã€title æˆ– class æ˜¯å¦åŒ…å«"ä¸‹ä¸€é¡µ"å…³é”®è¯
        for keyword in next_keywords:
            if keyword in link_text or keyword in link_title or keyword in link_class:
                next_url = urljoin(current_url, link['href'])
                return next_url

    return None


def scrape_images(url, page_num, total_pages, save_dir='images', log_callback=None):
    """ä»æŒ‡å®šURLæŠ“å–æ‰€æœ‰å›¾ç‰‡"""

    def log(msg):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        if log_callback:
            log_callback(msg)
        else:
            print(msg)

    # ä¼ªè£…æµè§ˆå™¨èº«ä»½ï¼ˆæ›´æ–°ä¸ºæœ€æ–° Chrome ç‰ˆæœ¬ï¼‰
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
    }

    try:
        # è·å–ç½‘é¡µå†…å®¹
        log(f"\n{'='*60}")
        log(f"[*] æ­£åœ¨æ”¶å‰²ç¬¬ {page_num}/{total_pages} é¡µ...")
        log(f"[+] URL: {url}")
        log(f"{'='*60}")

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # é˜²å°å°æŠ¤ç›¾ï¼šæ¨¡æ‹Ÿäººç±»æµè§ˆé€Ÿåº¦
        time.sleep(random.uniform(0.8, 1.5))

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # æå–æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
        img_tags = soup.find_all('img')

        if not img_tags:
            log("[!] æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
            return soup, 0

        log(f"[+] æ‰¾åˆ° {len(img_tags)} å¼ å›¾ç‰‡")

        # åˆ›å»ºä¿å­˜ç›®å½•
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            log(f"[+] å·²åˆ›å»ºç›®å½•: {save_dir}")

        # æå–å›¾ç‰‡é“¾æ¥å¹¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        img_urls = []
        for img in img_tags:
            img_url = img.get('src') or img.get('data-src')
            if img_url:
                # è½¬æ¢ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„
                absolute_url = urljoin(url, img_url)
                img_urls.append(absolute_url)

        # ä¸‹è½½å›¾ç‰‡
        success_count = 0
        for idx, img_url in enumerate(img_urls, 1):
            try:
                # è·å–æ–‡ä»¶å
                parsed_url = urlparse(img_url)
                filename = os.path.basename(parsed_url.path)

                # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–æ— æ‰©å±•åï¼Œä½¿ç”¨åºå·å‘½å
                if not filename or '.' not in filename:
                    filename = f"page{page_num}_image_{idx}.jpg"
                else:
                    # æ·»åŠ é¡µç å‰ç¼€é¿å…é‡å
                    name, ext = os.path.splitext(filename)
                    filename = f"page{page_num}_{name}{ext}"

                filepath = os.path.join(save_dir, filename)

                # ä¸‹è½½å›¾ç‰‡æ—¶æ·»åŠ  Referer é˜²æ­¢é˜²ç›—é“¾æ‹¦æˆª
                download_headers = headers.copy()
                download_headers['Referer'] = url

                img_response = requests.get(img_url, headers=download_headers, timeout=10)
                img_response.raise_for_status()

                # ä¿å­˜å›¾ç‰‡
                with open(filepath, 'wb') as f:
                    f.write(img_response.content)

                success_count += 1
                log(f"[>>] ç¬¬{page_num}é¡µ ä¸‹è½½è¿›åº¦: {idx}/{len(img_urls)} - {filename}")

                # é˜²å°å°æŠ¤ç›¾ï¼šæ¯ä¸‹è½½å‡ å¼ å›¾ç‰‡å°±ä¼‘æ¯ä¸€ä¸‹
                if idx % 5 == 0:
                    time.sleep(random.uniform(0.3, 0.8))

            except Exception as e:
                log(f"[X] ä¸‹è½½å¤±è´¥ [{img_url}]: {str(e)}")
                continue

        log(f"[OK] ç¬¬ {page_num} é¡µå®Œæˆï¼æˆåŠŸä¸‹è½½ {success_count}/{len(img_urls)} å¼ å›¾ç‰‡")

        return soup, success_count

    except requests.exceptions.RequestException as e:
        log(f"[X] è®¿é—®ç½‘é¡µå¤±è´¥: {str(e)}")
        return None, 0
    except Exception as e:
        log(f"[X] å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None, 0


def main():
    print("=" * 60)
    print(">> å›¾ç‰‡çˆ¬è™«å·¥å…· - æŒ‚æœºæ¨¡å¼")
    print("=" * 60)

    # è·å–åˆå§‹URL
    url = input("\nè¯·è¾“å…¥åˆå§‹ç½‘é¡µçš„URL: ").strip()

    if not url:
        print("[X] URLä¸èƒ½ä¸ºç©º")
        return

    # ç¡®ä¿URLåŒ…å«åè®®
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url

    # è·å–è¦æŠ“å–çš„æ€»é¡µæ•°
    while True:
        try:
            total_pages = int(input("è¯·è¾“å…¥è¦è‡ªåŠ¨æŠ“å–çš„æ€»é¡µæ•°ï¼ˆä¾‹å¦‚ 3ï¼‰: ").strip())
            if total_pages <= 0:
                print("[X] é¡µæ•°å¿…é¡»å¤§äº0ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("[X] è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

    print(f"\n[*] å¼€å§‹æŒ‚æœºæ¨¡å¼ï¼šå°†è‡ªåŠ¨æŠ“å– {total_pages} é¡µ")
    print("[*] é˜²å°å°æŠ¤ç›¾å·²å¯åŠ¨ï¼Œæ¨¡æ‹Ÿäººç±»æµè§ˆé€Ÿåº¦...")

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = 'images'

    # å¼€å§‹è‡ªåŠ¨ç¿»é¡µæŠ“å–
    current_url = url
    total_images = 0

    for page_num in range(1, total_pages + 1):
        # æŠ“å–å½“å‰é¡µ
        soup, success_count = scrape_images(current_url, page_num, total_pages, save_dir)
        total_images += success_count

        if soup is None:
            print(f"\n[!] ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥ï¼Œåœæ­¢ç¿»é¡µ")
            break

        # å¦‚æœè¿˜æœ‰ä¸‹ä¸€é¡µï¼ŒæŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
        if page_num < total_pages:
            print(f"\n[?] æ­£åœ¨æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥...")
            next_url = find_next_page_link(soup, current_url)

            if next_url:
                print(f"[+] æ‰¾åˆ°ä¸‹ä¸€é¡µ: {next_url}")
                current_url = next_url

                # é˜²å°å°æŠ¤ç›¾ï¼šç¿»é¡µå‰ä¼‘æ¯ä¸€ä¸‹
                wait_time = random.uniform(1.5, 3.0)
                print(f"[Z] ä¼‘æ¯ {wait_time:.1f} ç§’åç»§ç»­...")
                time.sleep(wait_time)
            else:
                print(f"[!] æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå·²æŠ“å– {page_num} é¡µååœæ­¢")
                break

    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"[OK] æŒ‚æœºå®Œæˆï¼")
    print(f"[+] æ€»å…±æˆåŠŸä¸‹è½½ {total_images} å¼ å›¾ç‰‡åˆ° {save_dir} ç›®å½•")
    print(f"{'='*60}")


class CyberScraperGUI:
    """4Kå£çº¸èµ›åšæ”¶å‰²æœº GUI"""

    def __init__(self, root):
        self.root = root
        self.root.title("4Kå£çº¸èµ›åšæ”¶å‰²æœº V1.0")
        self.root.geometry("800x600")
        self.root.resizable(True, True)

        # è®¾ç½®èµ›åšé£æ ¼é…è‰²
        bg_color = "#0a0e27"
        fg_color = "#00ff41"
        button_color = "#1a1f3a"

        self.root.configure(bg=bg_color)

        # æ˜¯å¦æ­£åœ¨è¿è¡Œ
        self.is_running = False

        # æ ‡é¢˜
        title_label = tk.Label(
            root,
            text="ğŸ”¥ 4Kå£çº¸èµ›åšæ”¶å‰²æœº V1.0 ğŸ”¥",
            font=("Consolas", 18, "bold"),
            bg=bg_color,
            fg=fg_color
        )
        title_label.pack(pady=15)

        # è¾“å…¥æ¡†åŒºåŸŸ
        input_frame = tk.Frame(root, bg=bg_color)
        input_frame.pack(pady=10, padx=20, fill="x")

        # ç›®æ ‡ç½‘å€
        url_label = tk.Label(
            input_frame,
            text="ç›®æ ‡ç½‘å€:",
            font=("Consolas", 11),
            bg=bg_color,
            fg=fg_color
        )
        url_label.grid(row=0, column=0, sticky="w", pady=5)

        self.url_entry = tk.Entry(
            input_frame,
            font=("Consolas", 10),
            bg=button_color,
            fg=fg_color,
            insertbackground=fg_color,
            width=60
        )
        self.url_entry.insert(0, "https://pic.netbian.com/4kfengjing/")
        self.url_entry.grid(row=0, column=1, pady=5, padx=10)

        # æŠ“å–é¡µæ•°
        pages_label = tk.Label(
            input_frame,
            text="æŠ“å–é¡µæ•°:",
            font=("Consolas", 11),
            bg=bg_color,
            fg=fg_color
        )
        pages_label.grid(row=1, column=0, sticky="w", pady=5)

        self.pages_entry = tk.Entry(
            input_frame,
            font=("Consolas", 10),
            bg=button_color,
            fg=fg_color,
            insertbackground=fg_color,
            width=60
        )
        self.pages_entry.insert(0, "3")
        self.pages_entry.grid(row=1, column=1, pady=5, padx=10)

        # å¼€å§‹æ”¶å‰²æŒ‰é’®
        self.start_button = tk.Button(
            root,
            text="âš¡ å¼€å§‹æ”¶å‰² âš¡",
            font=("Consolas", 14, "bold"),
            bg="#ff0066",
            fg="white",
            activebackground="#cc0052",
            activeforeground="white",
            command=self.start_scraping,
            cursor="hand2",
            height=2,
            width=20
        )
        self.start_button.pack(pady=15)

        # æ—¥å¿—åŒºåŸŸæ ‡ç­¾
        log_label = tk.Label(
            root,
            text="ğŸ“¡ å®æ—¶æ—¥å¿—",
            font=("Consolas", 12, "bold"),
            bg=bg_color,
            fg=fg_color
        )
        log_label.pack(pady=(10, 5))

        # æ—¥å¿—æ»šåŠ¨æ–‡æœ¬æ¡†
        self.log_text = scrolledtext.ScrolledText(
            root,
            font=("Consolas", 9),
            bg="#0d1117",
            fg="#00ff41",
            insertbackground=fg_color,
            wrap=tk.WORD,
            height=20
        )
        self.log_text.pack(pady=10, padx=20, fill="both", expand=True)

        # åˆå§‹æ¬¢è¿ä¿¡æ¯
        self.log("=" * 80)
        self.log("æ¬¢è¿ä½¿ç”¨ 4Kå£çº¸èµ›åšæ”¶å‰²æœº V1.0")
        self.log("è¯·è¾“å…¥ç›®æ ‡ç½‘å€å’ŒæŠ“å–é¡µæ•°ï¼Œç„¶åç‚¹å‡»ã€å¼€å§‹æ”¶å‰²ã€‘æŒ‰é’®")
        self.log("=" * 80)

    def log(self, message):
        """çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—è¾“å‡º"""
        def append():
            self.log_text.insert(tk.END, message + "\n")
            self.log_text.see(tk.END)

        # ä½¿ç”¨ after ç¡®ä¿åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–° GUI
        self.root.after(0, append)

    def start_scraping(self):
        """å¼€å§‹æŠ“å–ï¼ˆåœ¨åå°çº¿ç¨‹è¿è¡Œï¼‰"""
        if self.is_running:
            messagebox.showwarning("è­¦å‘Š", "æ”¶å‰²æœºæ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆï¼")
            return

        # è·å–è¾“å…¥
        url = self.url_entry.get().strip()
        pages_str = self.pages_entry.get().strip()

        # éªŒè¯è¾“å…¥
        if not url:
            messagebox.showerror("é”™è¯¯", "ç›®æ ‡ç½‘å€ä¸èƒ½ä¸ºç©ºï¼")
            return

        # ç¡®ä¿URLåŒ…å«åè®®
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        try:
            total_pages = int(pages_str)
            if total_pages <= 0:
                messagebox.showerror("é”™è¯¯", "é¡µæ•°å¿…é¡»å¤§äº0ï¼")
                return
        except ValueError:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥æœ‰æ•ˆçš„é¡µæ•°ï¼")
            return

        # ç¦ç”¨æŒ‰é’®
        self.start_button.config(state="disabled", text="â³ æ”¶å‰²ä¸­...")
        self.is_running = True

        # æ¸…ç©ºæ—¥å¿—
        self.log_text.delete(1.0, tk.END)

        # åœ¨åå°çº¿ç¨‹è¿è¡Œçˆ¬è™«
        thread = threading.Thread(
            target=self.run_scraper,
            args=(url, total_pages),
            daemon=True
        )
        thread.start()

    def run_scraper(self, url, total_pages):
        """åå°çº¿ç¨‹è¿è¡Œçš„çˆ¬è™«é€»è¾‘"""
        try:
            self.log(f"\n[*] å¼€å§‹æŒ‚æœºæ¨¡å¼ï¼šå°†è‡ªåŠ¨æŠ“å– {total_pages} é¡µ")
            self.log("[*] é˜²å°å°æŠ¤ç›¾å·²å¯åŠ¨ï¼Œæ¨¡æ‹Ÿäººç±»æµè§ˆé€Ÿåº¦...")

            # åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = 'images'

            # å¼€å§‹è‡ªåŠ¨ç¿»é¡µæŠ“å–
            current_url = url
            total_images = 0

            for page_num in range(1, total_pages + 1):
                # æŠ“å–å½“å‰é¡µ
                soup, success_count = scrape_images(
                    current_url,
                    page_num,
                    total_pages,
                    save_dir,
                    log_callback=self.log
                )
                total_images += success_count

                if soup is None:
                    self.log(f"\n[!] ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥ï¼Œåœæ­¢ç¿»é¡µ")
                    break

                # å¦‚æœè¿˜æœ‰ä¸‹ä¸€é¡µï¼ŒæŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
                if page_num < total_pages:
                    self.log(f"\n[?] æ­£åœ¨æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥...")
                    next_url = find_next_page_link(soup, current_url)

                    if next_url:
                        self.log(f"[+] æ‰¾åˆ°ä¸‹ä¸€é¡µ: {next_url}")
                        current_url = next_url

                        # é˜²å°å°æŠ¤ç›¾ï¼šç¿»é¡µå‰ä¼‘æ¯ä¸€ä¸‹
                        wait_time = random.uniform(1.5, 3.0)
                        self.log(f"[Z] ä¼‘æ¯ {wait_time:.1f} ç§’åç»§ç»­...")
                        time.sleep(wait_time)
                    else:
                        self.log(f"[!] æœªæ‰¾åˆ°ä¸‹ä¸€é¡µé“¾æ¥ï¼Œå·²æŠ“å– {page_num} é¡µååœæ­¢")
                        break

            # æœ€ç»ˆç»Ÿè®¡
            self.log(f"\n{'='*80}")
            self.log(f"[OK] æŒ‚æœºå®Œæˆï¼")
            self.log(f"[+] æ€»å…±æˆåŠŸä¸‹è½½ {total_images} å¼ å›¾ç‰‡åˆ° {save_dir} ç›®å½•")
            self.log(f"{'='*80}")

            # æ˜¾ç¤ºå®Œæˆå¯¹è¯æ¡†
            self.root.after(0, lambda: messagebox.showinfo(
                "æ”¶å‰²å®Œæˆ",
                f"æˆåŠŸä¸‹è½½ {total_images} å¼ å›¾ç‰‡åˆ° {save_dir} ç›®å½•ï¼"
            ))

        except Exception as e:
            self.log(f"\n[X] å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
            self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"å‘ç”Ÿé”™è¯¯: {str(e)}"))

        finally:
            # æ¢å¤æŒ‰é’®
            self.is_running = False
            self.root.after(0, lambda: self.start_button.config(
                state="normal",
                text="âš¡ å¼€å§‹æ”¶å‰² âš¡"
            ))


def launch_gui():
    """å¯åŠ¨ GUI ç•Œé¢"""
    root = tk.Tk()
    app = CyberScraperGUI(root)
    root.mainloop()


if __name__ == '__main__':
    # å¯åŠ¨ GUI ç•Œé¢
    launch_gui()
